# SoC'25 – ChatGPT From Scratch

## Mentee: Chirantan (24b0402)  
**Mentor:** Shivtej Ghatage

## 📌 Project Summary
This project aims to implement a simplified version of ChatGPT from scratch using transformers and PyTorch. The first half of the project focused on gaining deep theoretical knowledge about how LLMs work, and building basic familiarity with GitHub.

## 🗓️ Weekly Progress

### Week 1 – Transformers Overview
- Watched: [The Illustrated Transformer](https://youtu.be/kCc8FmEb1nY)
- Learned about: encoder-decoder structure, self-attention, positional encoding

### Week 2 – Attention Mechanisms and Tokenization
- Watched: [Attention Is All You Need – Explained](https://youtu.be/U0s0f995w14)
- Topics: QKV matrices, multi-head attention, and tokenization basics

### Week 3 – nanoGPT and GPT-Style Models
- Watched: [nanoGPT by Karpathy](https://youtu.be/PaCmpygFfXo)
- Learned: decoder-only models, GPT architecture

### Week 4 – Dataset Preparation and Training
- Watched: [Train GPT from Scratch](https://youtu.be/ZvhflmQKRgs)
- Understood: data loading, training loops, and model configuration

### Bonus – Learning Git & GitHub
- Watched: [Git & GitHub Full Course by Apna College](https://youtu.be/apGV9Kg7ics)
- Learned how to:
  - Create repositories
  - Push/pull changes
  - Edit README files
  - Manage basic version control

## ✅ What’s Next
- Begin model implementation with PyTorch
- Tokenizer integration and dataset selection
- Training and evaluation

> Coding will begin in the second half of the project.

## 📁 Repo Structure
- README: This file
- Code & notebooks: To be added post midterm

